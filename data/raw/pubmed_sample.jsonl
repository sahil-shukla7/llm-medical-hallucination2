{"pmid": "41395980", "title": "Artificial Intelligence in Anaesthesiology: Current Applications, Challenges, and Future Directions.", "abstract": "Artificial intelligence (AI) is rapidly transforming anaesthesiology through advances in machine learning, deep learning, and large language models. AI-driven tools now contribute to nearly every phase of perioperative care, including preoperative risk stratification, intraoperative monitoring, imaging interpretation, airway assessment, regional anaesthesia, and critical care. Applications such as automated American Society of Anesthesiologists classification, prediction of postoperative complications and intensive care unit needs, electroencephalography-based depth-of-anaesthesia estimation, and proactive haemodynamic management are reshaping clinical decision-making. AI-augmented echocardiography enhances chamber recognition and functional measurements, whereas computer vision systems support airway evaluation and ultrasound-guided regional anaesthesia by providing real-time anatomical identification and facilitating training. In critical care, AI models facilitate the early detection of sepsis, organ dysfunction, and haemodynamic instability, while improving workflow efficiency and resource allocation. AI is increasingly used in academic writing, data processing, and medical education, offering opportunities for personalised learning and simulation but raising concerns about accuracy and hallucinations. In this review, we aimed to summarise the current applications of AI in anaesthesiology, highlight the methodological, ethical, and practical challenges that limit its integration, and discuss future directions for its safe and effective adoption in perioperative care.", "journal": "Turkish journal of anaesthesiology and reanimation", "year": "2025"}
{"pmid": "41389261", "title": "Integrity meets innovation: A first principles approach to the ethics of AI utilization in medical research writing.", "abstract": "The integration of artificial intelligence (AI), particularly large language models (LLM), into medical research writing is reshaping the landscape of academic authorship, productivity, and scholarly merit. It has been demonstrated that LLM are capable of greatly expediting the process of researching, drafting, and publishing manuscripts, despite current limitations currently necessitating intensive human oversight to ensure veracity and mitigate the phenomenon of \"hallucination\". With these limitations being addressed by AI developers and perhaps on their way to irrelevance, a different question emerges as the most, and perhaps only, important one. This paper adopts a first-principles ethical approach to examine the core moral question: independent of technological feasibility, to what extent is it ethically permissible to use AI in the drafting of medical research? We argue that the ethical imperative to accelerate scientific discovery, especially in Medicine, outweighs traditional concerns about the mechanics of authorship and merit attribution. Drawing on Aristotelian teleological reasoning, we contend that the primary value of research lies not in the process of its composition but in its capacity to alleviate suffering and advance human knowledge. Further, we understand authorship as inherently human, as only humans possess the moral agency required to accept responsibility for their work, which is something AI, by its nature, lacks. The paper concludes with a set of normative recommendations to guide the responsible and transparent integration of LLM in research.", "journal": "Hellenic journal of nuclear medicine", "year": "2025"}
{"pmid": "41370789", "title": "Information Extraction of Doctoral Theses Using Two Different Large Language Models vs Health Services Researchers: Development and Usability Study.", "abstract": "The Archive of German-Language General Practice (ADAM) stores about 500 paper-based doctoral theses published from 1965 to today. Although they have been grouped in different categories, no deeper systematic process of information extraction (IE) has been performed yet. Recently developed large language models (LLMs) like ChatGPT have been attributed the potential to help in the IE of medical documents. However, there are concerns about LLM hallucinations. Furthermore, there have not been reports regarding their usage in nonrecent doctoral theses yet. The aim of this study is to analyze if LLMs can help to extract information from doctoral theses by using GPT-4o and Gemini-1.5-Flash for paper-based doctoral theses in ADAM. We randomly selected 10 doctoral theses published between 1965 and 2022. After preprocessing, we used two different LLM pipelines, using models by OpenAI and Google. Pipelines were used to extract dissertation characteristics and generate uniform abstracts. Furthermore, one pooled human-generated abstract was written for comparison. Furthermore, blinded raters were asked to evaluate LLM-generated abstracts in comparison to the human-generated ones. Bidirectional encoder representations from transformers scores were calculated as the evaluation metric. Relevant dissertation characteristics and keywords could be extracted for all theses (n=10): institute name and location, thesis title, author name(s), and publication year. For all except one doctoral thesis, an abstract could be generated using GPT-4o, while Gemini-1.5-Flash provided abstracts in all cases (n=10). The modality of abstract generation showed no influence in raters' evaluation using the nonparametric Kruskal-Wallis test for independent groups (P=.44). The creation of LLM-generated abstracts was estimated to be 24-36 times faster than creation by humans. Evaluation metrics showed moderate-to-high semantic similarity (mean bidirectional encoder representations from transformers F1-score, GPT-4o: 0.72 and Gemini: 0.71). Translation from German into English did not result in a loss of information (n=10). An accumulating body of unpublished doctoral theses makes it difficult to extract relevant evidence. Recent advances in LLMs like ChatGPT have raised expectations in text mining, but they have not yet been used in the IE of \"historic\" medical documents. This feasibility study suggests that both models (GPT-4o and Gemini-1.5-Flash) helped to accurately simplify and condense doctoral theses into relevant information, while LLM-generated abstracts were perceived as similar to human-generated ones, were semanticly similar, and took about 30 times less time to create. This pilot study demonstrates the feasibility of a regular office-scanning workflow and use of general-purpose LLMs to extract relevant information and produce accurate abstracts from ADAM doctoral theses. Taken together, this information could help researchers to better search the family medicine scientific literature over the last 60 years, helping to develop current research questions.", "journal": "JMIR formative research", "year": "2025"}
{"pmid": "41367595", "title": "Nursing Retrieval-Augmented Generation: Retrieval augmented generation for nursing question answering with large language models.", "abstract": "This study aimed to develop a Nursing Retrieval-Augmented Generation (NurRAG) system based on large language models (LLMs) and to evaluate its accuracy and clinical applicability in nursing question answering. A multidisciplinary team consisting of nursing experts, artificial intelligence researchers, and information engineers collaboratively designed the NurRAG framework following the principles of retrieval-augmented generation. The system included four functional modules: 1) construction of a nursing knowledge base through document normalization, embedding, and vector indexing; 2) nursing question filtering using a supervised classifier; 3) semantic retrieval and re-ranking for evidence selection; and 4) evidence-conditioned language model generation to produce citation-based nursing answers. The system was securely deployed on hospital intranet servers using Docker containers. Performance evaluation was conducted with 1,000 expert-verified nursing question-answer pairs. Semantic fidelity was assessed using Recall Oriented Understudy for Gisting Evaluation - Longest Common Subsequence (ROUGE-L), and clinical correctness was measured using Accuracy. The NurRAG system achieved significant improvements in both semantic fidelity and answer accuracy compared with conventional large language models. For ChatGLM2-6B, ROUGE-L increased from (30.73 \u00b1 1.48) % to (64.27 \u00b1 0.27) %, and accuracy increased from (49.08 \u00b1 0.92) % to (75.83 \u00b1 0.35) %. For LLaMA2-7B, ROUGE-L increased from (28.76 \u00b1 0.89) % to (60.33 \u00b1 0.21) %, and accuracy increased from (43.27 \u00b1 0.83) % to (73.29 \u00b1 0.33) %. All differences were statistically significant (<i>P</i> < 0.001). A quantitative case analysis further demonstrated that NurRAG effectively reduced hallucinated outputs and generated evidence-based, guideline-concordant nursing responses. The NurRAG system integrates domain-specific retrieval with LLMs generation to provide accurate, reliable, and traceable evidence-based nursing answers. The findings demonstrate the system's feasibility and potential to improve the accuracy of clinical knowledge access, support evidence-based nursing decision-making, and promote the safe application of artificial intelligence in nursing practice.", "journal": "International journal of nursing sciences", "year": "2025"}
{"pmid": "41364573", "title": "Refine Medical Diagnosis Using Generation Augmented Retrieval and Clinical Practice Guidelines.", "abstract": "Current medical language models, adapted from large language models, typically predict ICD code-based diagnosis from electronic health records (EHRs) because these labels are readily available. However, ICD codes do not capture the nuanced, context-rich reasoning clinicians use for diagnosis. Clinicians synthesize diverse patient data and reference clinical practice guidelines (CPGs) to make evidence-based decisions. This misalignment limits the clinical utility of existing models. We introduce GARMLE-G, a Generation-Augmented Retrieval framework that grounds medical language model outputs in authoritative CPGs. Unlike conventional Retrieval-Augmented Generation based approaches, GARMLE-G enables hallucination-free outputs by directly retrieving authoritative guideline content without relying on model-generated text. It (1) integrates LLM predictions with EHR data to create semantically rich queries, (2) retrieves relevant CPG knowledge snippets via embedding similarity, and (3) fuses guideline content with model output to generate clinically aligned recommendations. A prototype system for hypertension and coronary heart disease diagnosis was developed and evaluated on multiple metrics, demonstrating superior retrieval precision, semantic relevance, and clinical guideline adherence compared to RAG-based baselines, while maintaining a lightweight architecture suitable for localized healthcare deployment. This work provides a scalable, low-cost, and hallucination-free method for grounding medical language models in evidence-based clinical practice, with strong potential for broader clinical deployment.", "journal": "IEEE journal of biomedical and health informatics", "year": "2025"}
{"pmid": "41357426", "title": "Blinded evaluation of GPT-4 and physician responses to patient inquiries across multiple specialties.", "abstract": "To evaluate the quality of responses generated by GPT-4 in comparison to those written by hospital specialists and general practice physicians across multiple medical specialties. The goal was to assess whether large language models (LLMs) can support patient communication by providing accurate, useful, complete, and empathetic responses to real-world patient inquiries. We collected 100 anonymized patient questions from a public online health forum covering five specialties: cardiology, infectious diseases, neurology, gynecology, and gastroenterology. Responses were generated by GPT-4, 50 hospital-based specialists, and 50 general practice physicians. A group of 50 specialists, blinded to response source, evaluated each answer using four 7-point Likert scales: accuracy, usefulness, completeness, and empathy. The study design and reporting were informed by the CONSORT-AI Extension to promote transparency in AI evaluation. The model received significantly higher ratings across all four categories compared to both physician groups. It was ranked best in 67% of evaluations, particularly outperforming physicians in completeness and empathy. While response length correlated positively with quality for physicians, model's longer responses were less useful when overly detailed. The model consistently produced longer, more comprehensive replies than human groups. The model's strong performance in completeness and empathy highlights its potential role in enhancing patient communication. Although it matched or exceeded physicians in accuracy and usefulness, caution is warranted due to risks like hallucinations and lack of true understanding. This blinded evaluation suggests that AI-generated responses may support clinical practice by delivering accurate, comprehensive, and empathetic information for patient communication.", "journal": "Digital health", "year": "2025"}
{"pmid": "41354665", "title": "CASSIA: a multi-agent large language model for automated and interpretable cell annotation.", "abstract": "Cell type annotation is an essential step in single-cell RNA-sequencing analysis, and numerous annotation methods are available. Most require a combination of computational and domain-specific expertise, and they frequently yield inconsistent results that can be challenging to interpret. Large language models have the potential to expand accessibility while reducing manual input and improving accuracy, but existing approaches suffer from hyperconfidence, hallucinations, and lack of reasoning. To address these limitations, we developed CASSIA for automated, accurate, and interpretable cell annotation of single-cell RNA-sequencing data. As demonstrated in analyses of 970 cell types, CASSIA improves annotation accuracy in benchmark datasets as well as complex and rare cell populations, and also provides users with reasoning and quality assessment to ensure interpretability, guard against hallucinations, and calibrate confidence.", "journal": "Nature communications", "year": "2025"}
{"pmid": "41354552", "title": "Automatic Minds: Cognitive Parallels Between Hypnotic States and Large Language Model Processing.", "abstract": "The cognitive processes of the hypnotized mind and the computational operations of large language models (LLMs) share deep functional parallels. Both systems generate sophisticated, contextually appropriate behavior through automatic pattern-completion mechanisms operating with limited or unreliable executive oversight. This review examines this convergence across three principles: automaticity, in which responses emerge from associative rather than deliberative processes; suppressed monitoring, leading to errors such as confabulation in hypnosis and hallucination in LLMs; and heightened contextual dependency, where immediate cues-a therapist's suggestion or a user's prompt-override stable knowledge. These mechanisms reveal an observer-relative meaning gap: both systems produce coherent but ungrounded outputs that require an external interpreter to supply meaning. Hypnosis and LLMs also exemplify functional agency-the capacity for complex, goal-directed, context-sensitive behavior-without subjective agency, the conscious awareness of intention and ownership that defines human action. This distinction clarifies how purposive behavior can emerge without self-reflective consciousness, governed instead by structural and contextual dynamics. Finally, both domains illuminate the phenomenon of scheming: automatic, goal-directed pattern generation that unfolds without reflective awareness. Hypnosis provides an experimental model for understanding how intention can become dissociated from conscious deliberation, offering insights into the hidden motivational dynamics of artificial systems. Recognizing these parallels suggests that the future of reliable artificial intelligence lies in hybrid architectures that integrate generative fluency with mechanisms of executive monitoring, an approach inspired by the complex, self-regulating architecture of the human mind.", "journal": "Cyberpsychology, behavior and social networking", "year": "2025"}
{"pmid": "41350380", "title": "Enhancing patient participation in emergency department through patient-Friendly clinical notes generated by large Language models.", "abstract": "Patient-centered care (PCC) emphasizes providing patients with clear information to support active participation in medical decision-making. However, the fast-paced nature of emergency departments (ED), coupled with communication barriers and varying health literacy, limits effective patient engagement. While large language models (LLMs) have shown potential in generating patient-friendly documents, their use in ED settings remains underexplored. This study aimed to develop LLM-generated patient-friendly clinical notes (PFCNs) that transform clinical notes into plain language, and to evaluate whether PFCNs could enhance patient participation in ED consultations. In this study, a total of 120 PFCNs were generated and evaluated, receiving high understandability ratings from both 10 clinicians and 20 patients (PEMAT score: 87.2%). Patients who used PFCNs during simulated ED consultations reported significantly higher participation levels compared to prior ED experiences (PPQ, P\u2009<\u20090.05). Qualitative data showed that PFCNs supported understanding, question preparation, emotional reassurance and improved relationships with clinicians, though concerns about hallucinations and integration into clinical workflows remained. These findings suggested that PFCNs generated by LLMs show promise for enhancing patient participation in ED consultations. Future work should address accuracy and explore real-world integration to support safe and effective deployment.", "journal": "Scientific reports", "year": "2025"}
{"pmid": "41348988", "title": "Rapid Clinical Evidence Explorer: A Generative Pre-Trained Transformer-Powered Tool for Automated Oncology Evidence Extraction.", "abstract": "The rapid expansion of scientific literature has made it increasingly challenging for clinicians and researchers to efficiently identify relevant evidence. While large language models (LLMs) offer promising solutions for automating literature review tasks, few tools support integrated workflows that enable trend analysis as well. This study aimed to develop and evaluate Rapid Clinical Evidence eXplorer (<i>RaCE-X</i>), a Generative Pre-trained Transformer (GPT)-based automated pipeline designed to streamline abstract screening, extract structured information, and visualize key trends in clinical research. We used GPT-4.1 mini to screen 865 PubMed abstracts based on predefined screening criteria. Structured information was then extracted from the 87 relevant abstracts based on a predefined information model covering nine fields. A gold standard data set was created through expert review to assess model performance. The extracted information was visualized through an interactive dashboard. Usability was evaluated using the Post-Study System Usability Questionnaire (PSSUQ) and open-ended feedback from five clinical research coordinators. RaCE-X demonstrated high screening performance (precision = 0.954, recall = 0.988, F1 = 0.971) and achieved strong average performance in information extraction (precision = 0.977, recall = 0.989, F1 = 0.983), with no hallucinations identified. Usability testing indicated generally positive feedback (overall PSSUQ score = 2.8), with users noting that RaCE-X was intuitive and effective for data interpretation. RaCE-X enables efficient GPT-based abstract screening, structured information extraction, and research trend exploration, thereby facilitating the summary of clinically relevant evidence from the biomedical literature. This study demonstrates the feasibility of using LLMs to reduce manual workload and accelerate evidence-based research practices.", "journal": "JCO clinical cancer informatics", "year": "2025"}
{"pmid": "41346991", "title": "Clinical applications of large language models in knee osteoarthritis: a systematic review.", "abstract": "Knee osteoarthritis (KOA) is a common chronic degenerative disease that significantly impacts patients' quality of life. With the rapid advancement of artificial intelligence, large language models (LLMs) have demonstrated potential in supporting medical information extraction, clinical decision-making, and patient education through their natural language processing capabilities. However, the current landscape of LLM applications in the KOA domain, along with their methodological quality, has yet to be systematically reviewed. Therefore, this systematic review aims to comprehensively summarize existing clinical studies on LLMs in KOA, evaluate their performance and methodological rigor, and identify current challenges and future research directions. Following the PRISMA guidelines, a systematic search was conducted in PubMed, Cochrane Library, Embase databases and Web of science for literature published up to June 2025. The protocol was preregistered on the OSF platform. Studies were screened using standardized inclusion and exclusion criteria. Key study characteristics and performance evaluation metrics were extracted. Methodological quality was assessed using tools such as Cochrane RoB, STROBE, STARD, and DISCERN. Additionally, the CLEAR-LLM and CliMA-10 frameworks were applied to provide complementary evaluations of quality and performance. A total of 16 studies were included, covering various LLMs such as ChatGPT, Gemini, and Claude. Application scenarios encompassed text generation, imaging diagnostics, and patient education. Most studies were observational in nature, and overall methodological quality ranged from moderate to high. Based on CliMA-10 scores, LLMs exhibited upper-moderate performance in KOA-related tasks. The ChatGPT-4 series consistently outperformed other models, especially in structured output generation, interpretation of clinical terminology, and content accuracy. Key limitations included insufficient sample representativeness, inconsistent control over hallucinated content, and the lack of standardized evaluation tools. Large language models show notable potential in the KOA field, but their clinical application is still exploratory and limited by issues such as sample bias and methodological heterogeneity. Model performance varies across tasks, underscoring the need for improved prompt design and standardized evaluation frameworks. With real-world data and ethical oversight, LLMs may contribute more significantly to personalized KOA management. https://osf.io/jy4kz, identifier 10.17605/OSF.IO/479R8.", "journal": "Frontiers in medicine", "year": "2025"}
{"pmid": "41339739", "title": "Benchmarking large language models on the United States medical licensing examination for clinical reasoning and medical licensing scenarios.", "abstract": "Artificial intelligence (AI) is transforming healthcare by assisting with intricate clinical reasoning and diagnosis. Recent research demonstrates that large language models (LLMs), such as ChatGPT and DeepSeek, possess considerable potential in medical comprehension. This study meticulously evaluates the clinical reasoning capabilities of four advanced LLMs, including ChatGPT, DeepSeek, Grok, and Qwen, utilizing the United States Medical Licensing Examination (USMLE) as a standard benchmark. We assess 376 publicly accessible USMLE sample exam questions (Step 1, Step 2 CK, Step 3) from the most recent booklet released in July 2023. We analyze model performance across four question categories: text-only, text with image, text with mathematical reasoning, and integrated text-image-mathematical reasoning and measure model accuracy at three USMLE steps. Our findings show that DeepSeek and ChatGPT consistently outperform Grok and Qwen, with DeepSeek reaching 93% on Step 2 CK. Error analysis revealed that universal failures were rare (\u22641.60%) and concentrated in multimodal and quantitative reasoning tasks, suggesting both ensemble potential and shared blind spots. Compared to the baseline ChatGPT-3.5 Turbo, newer models demonstrate substantial gains, though possible training-data exposure to USMLE content limits generalizability. Despite encouraging accuracy, models exhibited overconfidence and hallucinations, underscoring the need for human oversight. Limitations include reliance on sample questions, the small number of multimodal items, and lack of real-world datasets. Future work should expand benchmarks, integrate physician feedback, and improve reproducibility through shared prompts and configurations. Overall, these results highlight both the promise and the limitations of LLMs in medical testing: strong accuracy and complementarity, but persistent risks requiring innovation, benchmarking, and clinical oversight.", "journal": "Scientific reports", "year": "2025"}
{"pmid": "41337739", "title": "Feasibility of a Specialized Large Language Model for Postgraduate Medical Examination Preparation: Single-Center Proof-Of-Concept Study.", "abstract": "Large language models (LLMs) are increasingly used in medical education for feedback and grading; yet their role in postgraduate examination preparation remains uncertain due to inconsistent grading, hallucinations, and user acceptance. This study evaluates the Personalized Anesthesia Study Support (PASS), a specialized GPT-4 model developed to assist candidates preparing for Singapore's postgraduate specialist anesthesiology examination. We assessed user acceptance, grading interrater reliability, and hallucination detection rates to determine the feasibility of integrating specialized LLMs into high-stakes examination preparation. PASS was built on OpenAI's GPT-4 and adapted with domain-specific prompts and references. Twenty-one senior anesthesiology residents completed a mock short answer question examination, which was independently graded by 3 human examiners and 3 PASS iterations. Participants reviewed feedback from both PASS and standard GPT-4 and completed a technology acceptance model (TAM) survey. Grading reliability was evaluated using Cohen and Fleiss \u03ba. Hallucination rates were assessed by participants and examiners. Of the 21 participants, 17 (81%) completed the TAM survey, generating 136 responses. PASS scored significantly higher than standard GPT-4 in usefulness (mean 4.25, SD 0.50 vs mean 3.44, SD 0.82; P<.001), efficiency (mean 4.12, SD 0.61 vs mean 3.41, SD 0.74; P<.001), and likelihood of future use (mean 4.13, SD 0.75 vs mean 3.59, SD 0.90; P<.001), with no significant difference in ease of use (mean 4.56, SD 0.63 vs mean 4.50, SD 0.61; P=.35). Internal grading reliability was moderate for PASS (\u03ba=0.522) and fair for human examiners (\u03ba=0.275). Across 316 PASS-generated responses, 67 hallucinations and 189 deviations were labeled. Hallucination labeling rates were comparable between candidates (10/67, 15%) and examiners (57/249, 22.9%; P=.21), while examiners labeled significantly more deviations (168/249, 67.5% vs 21/67, 31%; P<.001). PASS demonstrated strong user acceptance and grading reliability, suggesting feasibility in high-stakes examination preparation. Experienced learners could identify major hallucinations at comparable rates to examiners, suggesting potential in self-directed learning but with continued need for caution. Further research should refine grading accuracy and explore multicenter evaluation of specialized LLMs for postgraduate medical education.", "journal": "JMIR formative research", "year": "2025"}
{"pmid": "41329953", "title": "Enhancing Large Language Models for Improved Accuracy and Safety in Medical Question Answering: Comparative Study.", "abstract": "Large language models (LLMs) offer the potential to improve virtual patient-physician communication and reduce health care professionals' workload. However, limitations in accuracy, outdated knowledge, and safety issues restrict their effective use in real clinical settings. Addressing these challenges is crucial for making LLMs a reliable health care tool. This study aimed to evaluate the efficacy of Med-RISE, an information retrieval and augmentation tool, in comparison with baseline LLMs, focusing on enhancing accuracy and safety in medical question answering across diverse clinical domains. This comparative study introduces Med-RISE, an enhanced version of a retrieval-augmented generation framework specifically designed to improve question-answering performance across wide-ranging medical domains and diverse disciplines. Med-RISE consists of 4 key steps: query rewriting, information retrieval (providing local and real-time retrieval), summarization, and execution (a fact and safety filter before output). This study integrated Med-RISE with 4 LLMs (GPT-3.5, GPT-4, Vicuna-13B, and ChatGLM-6B) and assessed their performance on 4 multiple-choice medical question datasets: MedQA (US Medical Licensing Examination), PubMedQA (original and revised versions), MedMCQA, and EYE500. Primary outcome measures included answer accuracy and hallucination rates, with hallucinations categorized into factuality (inaccurate information) or faithfulness (inconsistency with instructions) types. This study was conducted between March 2024 and August 2024. The integration of Med-RISE with each LLM led to a substantial increase in accuracy, with improvements ranging from 9.8% to 16.3% (mean 13%, SD 2.3%) across the 4 datasets. The enhanced accuracy rates were 16.3%, 12.9%, 13%, and 9.8% for GPT-3.5, GPT-4, Vicuna-13B, and ChatGLM-6B, respectively. In addition, Med-RISE effectively reduced hallucinations, with reductions ranging from 11.8% to 18% (mean 15.1%, SD 2.8%), factuality hallucinations decreasing by 13.5%, and faithfulness hallucinations decreasing by 5.8%. The hallucination rate reductions were 17.7%, 12.8%, 18%, and 11.8% for GPT-3.5, GPT-4, Vicuna-13B, and ChatGLM-6B, respectively. The Med-RISE framework significantly improves the accuracy and reduces the hallucinations of LLMs in medical question answering across benchmark datasets. By providing local and real-time information retrieval and fact and safety filtering, Med-RISE enhances the reliability and interpretability of LLMs in the medical domain, offering a promising tool for clinical practice and decision support.", "journal": "JMIR medical education", "year": "2025"}
{"pmid": "41325573", "title": "Reimagining Cancer Care With Generative Artificial Intelligence: The Promise of Large Language Models.", "abstract": "The emergence of state-of-the-art large language models (LLMs), which hold the ability to generalize to diverse natural language processing tasks, has led to new opportunities in health care. Oncology is especially well-suited to leverage these resources as the journeys of patients with cancer inherently yield extensive, longitudinal data sets comprising clinical narratives, pathology and radiology reports, and genomic sequencing reports. This review begins with an overview of the fundamental concepts behind LLMs, including the definitions, architecture, training paradigm, and performance optimization through prompt engineering and retrieval-augmented generation. We also take a moment to explore the newly emerging paradigm of LLMs in a multiagentic framework. We then synthesize current research on how LLMs may benefit stakeholders within the practice of oncology, including patients, oncologists, researchers, and learners. Finally, we address the limitations and risks of LLMs, including hallucinations, inherent biases, patient privacy, and clinician deskilling. While research thus far shows significant potential for LLMs to transform cancer care, necessary future directions include studies emphasizing patient stakeholder perspectives on LLM incorporation in clinical workflows, the development of relevant clinical benchmarks for LLM evaluation, a greater focus on real-world prospective testing, and deeper exploration of LLM reasoning capabilities.", "journal": "JCO clinical cancer informatics", "year": "2025"}
{"pmid": "41323158", "title": "Reasoning with large language models in medicine: a systematic review of techniques, challenges and clinical integration.", "abstract": "Large Language Models (LLMs) have emerged as transformative tools in healthcare, demonstrating unprecedented capabilities in medical reasoning tasks that require complex inference, pattern recognition, and decision-making under uncertainty. This comprehensive review examines the current state of LLMs applications in medical reasoning across diverse clinical contexts, including diagnostic reasoning, clinical decision support, medical imaging analysis, drug discovery, and patient management. We systematically analyze the methodological approaches used to adapt and evaluate LLMs, comparing their performance against traditional clinical decision support systems and human clinicians. We further provide a critical comparative analysis of architectural adaptations, fine-tuning techniques, and domain-specific evaluation protocols. Our review encompasses models such as GPT-4, PaLM, Med-PaLM, and BioGPT, highlighting how model design and training paradigms influence reasoning capabilities, generalization, and clinical applicability. We assess their ability to process multimodal data, generate hypotheses, and provide evidence-based recommendations. Distinct adaptation methods such as prompt engineering, few-shot learning, and reinforcement learning with human feedback are examined for their impact on medical accuracy and robustness. We identify key technical challenges including hallucinations, inherited biases, and accountability issues, along with ethical and deployment barriers. Unlike prior reviews, we emphasize open research problems in Artificial Intelligence (AI), including symbolic integration, and context-aware reasoning framing LLMs as computational systems that push the boundaries of interdisciplinary computer science. While LLMs offer promise for enhancing diagnostic accuracy and decision-making, substantial challenges remain. The review concludes by outlining future directions, including hybrid neuro-symbolic models, rigorous evaluation frameworks, and human-in-the-loop systems to ensure safe, transparent, and fair integration into healthcare. Rather than replacing clinicians, LLMs are best positioned as collaborative AI agents advancing the frontiers of intelligent, assistive technologies in medicine.", "journal": "Health information science and systems", "year": "2026"}
{"pmid": "41321931", "title": "Evaluating large language models' performance in answering common questions on drug-induced liver injury.", "abstract": "Drug-induced liver injury (DILI) is a complex condition often linked to medication behaviors, with patient education having a crucial role in optimizing outcomes. Large language models (LLMs) could serve as promising tools for scalable patient support, but their utility remains unclear. This study systematically evaluated the capability of six popular open- and closed-source LLMs in addressing common DILI-related queries, focusing on patient-centered education. Twenty-eight frequently asked DILI questions were collected with input from hepatologists and patients (n = 15), and categorized into six clinical domains. Responses from six LLMs (GPT-4, GPT-3.5, Claude-2, Claude-1.3, Gemini, and LLaMA-3.1-405B) were anonymized, randomized, and independently evaluated by three hepatologists for accuracy, comprehensiveness, and safety. Additional analyses included automated readability assessment, domain-specific analysis, detailed expert-led error analysis, and direct comparison with physician responses. LLaMA-3.1-405B achieved the highest performance across most domains, with mean accuracy, comprehensiveness, and safety scores of 8.18 \u00b1 1.68, 3.86 \u00b1 0.70, and 4.02 \u00b1 0.84, respectively, significantly surpassing other models (Dunn's <i>post hoc</i> test, all <i>p</i> <0.05). O1-preview ranked second (accuracy, 7.29 \u00b1 1.38; safety, 3.80 \u00b1 0.92), whereas GPT-3.5-Turbo consistently performed worst (accuracy, 4.61 \u00b1 1.17; comprehensiveness, 2.13 \u00b1 0.79). In direct comparison with physicians, both LLaMA-3.1-405B and o1-preview significantly outperformed residents and primary care physicians across all metrics (<i>p</i> <0.05). Error analysis showed that omission of crucial information accounted for 72% of errors, predominantly in GPT-3.5-Turbo, whereas hallucinations were rare (<10%) but notable in LLaMA outputs. This study represents the first systematic evaluation of LLMs for DILI-focused patient education. High-performing, publicly accessible LLMs demonstrate the potential to deliver accurate, comprehensive, and safe health information, even surpassing physician responses. DILI is a complex and multidisciplinary condition where patient understanding has a crucial role in management outcomes, yet educational resources remain scarce. By systematically evaluating six widely used LLMs, including both open- and closed-source models, this study provides new insights into the potential of artificial intelligence tools to enhance patient education and supplement clinical communication in hepatology. These findings are particularly important for physicians, patient educators, and healthcare policymakers seeking scalable and reliable strategies to support liver disease management. Although further refinement and clinical oversight are necessary to ensure content safety and accuracy, integrating LLM-based tools into patient education initiatives could offer a practical pathway to improve health literacy and engagement in real-world settings.", "journal": "JHEP reports : innovation in hepatology", "year": "2025"}
{"pmid": "41319470", "title": "Med-VCD: Mitigating hallucination for medical large vision language models through visual contrastive decoding.", "abstract": "Large vision-language models (LVLMs) are now central to healthcare applications such as medical visual question answering and imaging report generation. Yet, these models remain vulnerable to hallucination outputs that appear plausible but are in fact incorrect. In the natural image domain, several decoding strategies have been proposed to mitigate hallucinations by reinforcing visual evidence, but most rely on secondary decoding or rollback procedures that substantially slow inference. Moreover, existing solutions are often domain-specific and may introduce misalignment between modalities or between generated and ground-truth content. We introduce Med-VCD, a sparse visual-contrastive decoding method that mitigates hallucinations in medical LVLMs without the time overhead of secondary decoding. Med-VCD incorporates a novel token-sparsification strategy that selects visually informed tokens on the fly, trimming redundancy while retaining critical visual context and thus balancing efficiency with reliability. Evaluations on eight medical datasets, spanning ophthalmology, radiology, and pathology tasks in visual question answering, report generation, and dedicated hallucination benchmarks, show that Med-VCD raises factual accuracy by an average of 13% and improves hallucination accuracy by 6% relative to baseline medical LVLMs.", "journal": "Computers in biology and medicine", "year": "2025"}
{"pmid": "41319399", "title": "MedSumGraph: enhancing GraphRAG for medical QA with summarization and optimized prompts.", "abstract": "The rapid development of large language models (LLMs) has accelerated research into applying artificial intelligence (AI) to domains such as medical question answering and clinical decision support. However, LLMs face substantial limitations in medical contexts due to challenges in understanding specialized terminology, complex contextual information, hallucination issues (i.e., generating incorrect responses), and the black-box nature of their reasoning processes. To address these issues, methods like retrieval-augmented generation (RAG) and its graph-based variant, GraphRAG, have been proposed to incorporate external knowledge into LLMs. Nonetheless, these approaches often rely heavily on external resources and increase system complexity. In this study, we introduce MedSumGraph, a medical question-answering system that enhances GraphRAG by integrating structured medical knowledge summaries and optimized prompt designs. Our method enables LLMs to better interpret domain-specific knowledge without requiring additional training, and it enhances the reliability and interpretability of responses by directly embedding factual evidence and graph-based reasoning into the generation process. MedSumGraph achieves competitive performance on two out of eight multiple-choice medical QA benchmarks, including MedQA (USMLE), outperforming closed-source LLMs and domain-specific foundation models. Moreover, it generalizes effectively to open-domain QA tasks, yielding significant gains in reasoning over common knowledge and evaluating the truthfulness of answers. These findings demonstrate the potential of structured summarization and graph-based reasoning in enhancing the trustworthiness and versatility of LLM-driven medical AI systems.", "journal": "Artificial intelligence in medicine", "year": "2025"}
{"pmid": "41308189", "title": "GrantCheck-an AI Solution for Guiding Grant Language to New Policy Requirements: Development Study.", "abstract": "Academic institutions face increasing challenges in grant writing due to evolving federal and state policies that restrict the use of specific language. Manual review processes are labor-intensive and may delay submissions, highlighting the need for scalable, secure solutions that ensure compliance without compromising scientific integrity. This study aimed to develop a secure, artificial intelligence-powered tool that assists researchers in writing grants consistent with evolving state and federal policy requirements. GrantCheck (University of Massachusetts Chan Medical School) was built on a private Amazon Web Services virtual private cloud, integrating a rule-based natural language processing engine with large language models accessed via Amazon Bedrock. A hybrid pipeline detects flagged terms and generates alternative phrasing, with validation steps to prevent hallucinations. A secure web-based front end enables document upload and report retrieval. Usability was assessed using the System Usability Scale. GrantCheck achieved high performance in detecting and recommending alternatives for sensitive terms, with a precision of 1.00, recall of 0.73, and an F<sub>1</sub>-score of 0.84-outperforming general-purpose models including GPT-4o (OpenAI; F<sub>1</sub>=0.43), Deepseek R1 (High-Flyer; F<sub>1</sub>=0.40), Llama 3.1 (Meta AI; F<sub>1</sub>=0.27), Gemini 2.5 Flash (Google; F<sub>1</sub>=0.58), and even Gemini 2.5 Pro (Google; F<sub>1</sub>=0.72). Usability testing among 25 faculty and staff yielded a mean System Usability Scale score of 85.9 (SD 13.4), indicating high user satisfaction and strong workflow integration. GrantCheck demonstrates the feasibility of deploying institutionally hosted, artificial intelligence-driven systems to support compliant and researcher-friendly grant writing. Beyond administrative efficiency, such systems can indirectly safeguard public health research continuity by minimizing grant delays and funding losses caused by language-related policy changes. By maintaining compliance without suppressing scientific rigor or inclusivity, GrantCheck helps protect the pipeline of research that advances biomedical discovery, health equity, and patient outcomes. This capability is particularly relevant for proposals in sensitive domains-such as social determinants of health, behavioral medicine, and community-based research-that are most vulnerable to evolving policy restrictions. As a proof-of-concept development study, our implementation is tailored to one institution's policy environment and security infrastructure, and findings should be interpreted as preliminary rather than universally generalizable.", "journal": "JMIR formative research", "year": "2025"}
{"pmid": "41301175", "title": "The Development and Evaluation of a Retrieval-Augmented Generation Large Language Model Virtual Assistant for Postoperative Instructions.", "abstract": "During postoperative recovery, patients and their caregivers often lack crucial information, leading to numerous repetitive inquiries that burden healthcare providers. Traditional discharge materials, including paper handouts and patient portals, are often static, overwhelming, or underutilized, leading to patient overwhelm and contributing to unnecessary ER visits and overall healthcare overutilization. Conversational chatbots offer a solution, but Natural Language Processing (NLP) systems are often inflexible and limited in understanding, while powerful Large Language Models (LLMs) are prone to generating \"hallucinations\". To combine the deterministic framework of traditional NLP with the probabilistic capabilities of LLMs, we developed the AI Virtual Assistant (AIVA) Platform. This system utilizes a retrieval-augmented generation (RAG) architecture, integrating Gemini 2.0 Flash with a medically verified knowledge base via Google Vertex AI, to safely deliver dynamic, patient-facing postoperative guidance grounded in validated clinical content. The AIVA Platform was evaluated through 750 simulated patient interactions derived from 250 unique postoperative queries across 20 high-frequency recovery domains. Three blinded physician reviewers assessed formal system performance, evaluating classification metrics (accuracy, precision, recall, F1-score), relevance (SSI Index), completeness, and consistency (5-point Likert scale). Safety guardrails were tested with 120 out-of-scope queries and 30 emergency escalation scenarios. Additionally, groundedness, fluency, and readability were assessed using automated LLM metrics. The system achieved 98.4% classification accuracy (precision 1.0, recall 0.98, F1-score 0.9899). Physician reviews showed high completeness (4.83/5), consistency (4.49/5), and relevance (SSI Index 2.68/3). Safety guardrails successfully identified 100% of out-of-scope and escalation scenarios. Groundedness evaluations demonstrated strong context precision (0.951), recall (0.910), and faithfulness (0.956), with 95.6% verification agreement. While fluency and semantic alignment were high (BERTScore F1 0.9013, ROUGE-1 0.8377), readability was 11th-grade level (Flesch-Kincaid 46.34). The simulated testing demonstrated strong technical accuracy, safety, and clinical relevance in simulated postoperative care. Its architecture effectively balances flexibility and safety, addressing key limitations of standalone NLP and LLMs. While readability remains a challenge, these findings establish a solid foundation, demonstrating readiness for clinical trials and real-world testing within surgical care pathways.", "journal": "Bioengineering (Basel, Switzerland)", "year": "2025"}
{"pmid": "41293332", "title": "Artificial Intelligence and Medical Translation: An Editorial on the Ethical Considerations for Emerging Technologies in Dermatology.", "abstract": "The growing demand for medical translation services in the U.S. highlights the potential of artificial intelligence (AI), large language models (LLMs) like ChatGPT (OpenAI, San Francisco, CA), to bridge language gaps. However, their use in dermatology raises ethical concerns, including information accuracy, patient privacy, dialectical variations, legal accountability, and algorithm bias for a variety of skin colors. AI models may default to informal language, leading to misunderstandings, and their limited ability to handle less common dialects poses communication challenges. The risk of \"hallucination,\" where incorrect information is generated, and inadequate data oversight further complicate their use. In dermatology, precise translation is crucial due to the field's visual nature and the sensitivity of cosmetic concerns. Linguistic diversity can lead to misinterpretations, affecting patient care. Dermatologists must consider these ethical implications to ensure AI tools address the nuances of dermatological terminology and regional language differences, ultimately improving patient outcomes.", "journal": "Cureus", "year": "2025"}
{"pmid": "41289129", "title": "Extractive Radiology Reporting with Memory-based Cross-modal Representations.", "abstract": "Radiology report generation (RRG) produces detailed textual descriptions for radiographs, serving as a crucial task for medical analysis and diagnosis. Most existing RRG approaches naturally follow the multimodal text generation paradigm, where autoregressive models are utilized to perform token-by-token report generation and thus are potentially risky in generating invalid content while being limited in low information processing speed. Although advanced architectures, such as pre-trained models and large language models (LLMs), are applied for RRG and achieve good performance, they still face the aforementioned risk and speed limitation, especially that LLMs may introduce hallucinations. Consider that radiology reports are highly patternized, sentences in them convey specific meanings independently and are frequently reused, we propose a new extractive radiograph reporting (ERR) workflow and design a dedicated framework that efficiently and accurately extracts appropriate sentences from existing radiological cases for report generation. Our approach employs a memory module to store important medical information and enhance the encoding for input radiograph with better cross-modal representations, which are used to match sentences for the extraction process. We conducted experiments on two widely used benchmark datasets, with the results demonstrating that our approach outperforms strong baselines and achieves comparable results with existing state-of-the-art generative models. Analyses further confirm that our ERR approach not only produces reports with reliable content but also ensures high training and inference efficiency.", "journal": "IEEE transactions on medical imaging", "year": "2025"}
{"pmid": "41286838", "title": "Artificial intelligence in polycystic ovary syndrome: a systematic review of diagnostic and predictive applications.", "abstract": "Polycystic ovary syndrome (PCOS) is one of the most common endocrine disorders, affecting 8\u201313% of women of reproductive age. Its heterogeneous presentation and the variability of diagnostic criteria make accurate diagnosis and effective management challenging. Artificial intelligence (AI) methods, including machine learning (ML), deep learning (DL), explainable AI (XAI), and large language models (LLMs), have recently emerged as promising approaches to address these gaps. This systematic review aimed to provide a comprehensive synthesis of AI applications in PCOS, with emphasis on diagnostic performance, biomarker discovery, risk prediction, clinical decision support, model interpretability, and the emerging use of generative AI. Following PRISMA 2020 guidelines, PubMed, Scopus, and Web of Science were searched from inception to March 2025. Eligible studies applied AI techniques to PCOS and reported at least one performance metric. Two reviewers independently screened and extracted data, with quality appraisal conducted using QUADAS-2 and ROBIS. Given the heterogeneity of designs and outcomes, findings were narratively synthesized across imaging, clinical/EHR, and biomarker/-omics domains. From 662 retrieved records, 80 studies met the inclusion criteria. CNN-based models dominated imaging applications, with accuracies often exceeding 95% and occasionally reaching 98\u201399%. Supervised ML approaches, particularly random forests and support vector machines, achieved consistent high performance in clinical and biochemical datasets. Omics-based studies revealed novel biomarkers such as HDDC3, SDC2, MAP1LC3A, and OVGP1. However, only about one-quarter of studies applied XAI methods, limiting transparency and clinical trust. Early evaluations of LLMs suggested potential for patient education and decision support but highlighted risks of bias, hallucination, and lack of domain-specific training. Key limitations across studies included small sample sizes, class imbalance, methodological heterogeneity, and limited external validation. AI offers substantial opportunities to advance PCOS diagnosis and prediction by integrating multimodal data and reducing diagnostic subjectivity. Yet its clinical adoption is constrained by interpretability gaps and insufficient validation. Future priorities include large multicenter studies, standardized reporting, systematic use of XAI, and careful evaluation of LLMs to ensure safe, equitable, and clinically meaningful integration into PCOS care. The online version contains supplementary material available at 10.1186/s12911-025-03255-6.", "journal": "BMC medical informatics and decision making", "year": "2025"}
{"pmid": "41284992", "title": "Large Language Models in Critical Care Medicine: Scoping Review.", "abstract": "With the rapid development of artificial intelligence, large language models (LLMs) have shown strong capabilities in natural language understanding, reasoning, and generation, attracting much research interest in applying LLMs to health and medicine. Critical care medicine (CCM) provides diagnosis and treatment for patients with critical illness who often require intensive monitoring and interventions in intensive care units (ICUs). Whether LLMs can be applied to CCM, and whether they can operate as ICU experts in assisting clinical decision-making rather than \"stochastic parrots,\" remains uncertain. This scoping review aims to provide a panoramic portrait of the application of LLMs in CCM, identifying the advantages, challenges, and future potential of LLMs in this field. This study was conducted in accordance with the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews) guidelines. Literature was searched across 7 databases, including PubMed, Embase, Scopus, Web of Science, CINAHL, IEEE Xplore, and ACM Digital Library, from the first available paper to August 22, 2025. From an initial 2342 retrieved papers, 41 were selected for final review. LLMs played an important role in CCM through the following 3 main channels: clinical decision support, medical documentation and reporting, and medical education and doctor-patient communication. Compared to traditional artificial intelligence models, LLMs have advantages in handling unstructured data and do not require manual feature engineering. Meanwhile, applying LLMs to CCM has faced challenges, including hallucinations and poor interpretability, sensitivity to prompts, bias and alignment challenges, and privacy and ethical issues. Although LLMs are not yet ICU experts, they have the potential to become valuable tools in CCM, helping to improve patient outcomes and optimize health care delivery. Future research should enhance model reliability and interpretability, improve model training and deployment scalability, integrate up-to-date medical knowledge, and strengthen privacy and ethical guidelines, paving the way for LLMs to fully realize their impact in critical care. OSF Registries yn328; https://osf.io/yn328/.", "journal": "JMIR medical informatics", "year": "2025"}
{"pmid": "41282683", "title": "Combining Clinician Expertise with Prompt Engineering enhances Small Language Models Reliability for Cancer Entity Recognition in Electronic Health Records.", "abstract": "Real-world data (RWD), largely stored in unstructured electronic health records (EHRs), are critical for understanding complex diseases like cancer. However, extracting structured information from these narratives is challenging due to linguistic variability, semantic complexity, and privacy concerns. This study evaluates the performance of four locally deployable and small language models (SLMs), LLaMA, Mistral, BioMistral, and MedLLaMA, for information extraction (IE) from Italian EHRs within the APOLLO 11 trial on non-small cell lung cancer (NSCLC). We examined three prompting strategies (zero-shot, few-shot, and annotated few-shot) across English and Italian, involving clinicians with varying expertise to assess prompt design's impact on accuracy. Results show that general-purpose models (e.g., LLaMA 3.1 8B) outperform biomedical models in most tasks, particularly in extracting binary features. Multiclass variables such as TNM staging, PD-L1, and ECOG were more difficult due to implicit language and lack of standardization. Few-shot prompting and native-language inputs significantly improved performance and reduced hallucinations. Clinical expertise enhanced consistency in annotation, particularly among students using annotated examples. The study confirms that privacy-preserving SLMs can be deployed locally for efficient and secure cancer data extraction. Findings highlight the need for hybrid systems combining SLMs with expert input and underline the importance of aligning clinical documentation practices with SLM capabilities. This is the first study to benchmark SLMs on Italian EHRs and investigate the role of clinical expertise in prompt engineering, offering valuable insights for the future integration of SLMs into real-world clinical workflows.", "journal": "medRxiv : the preprint server for health sciences", "year": "2025"}
{"pmid": "41281608", "title": "Large language models in healthcare: a systematic evaluation on medical Q/A datasets.", "abstract": "This study systematically evaluates the performance of state-of-the-art large language models (LLMs) in medical and healthcare applications, focusing on their accuracy in answering domain-specific questions. Using benchmark medical question-answering datasets-PubMedQA, MedQA, and MedMCQA-we assess a diverse set of LLMs, including GPT-4, Med-PaLM-2, OpenBioLLM, BioMistral, MediTron, MedAlpaca, and AlpaCare. Our analysis highlights the varying capabilities of these models across different datasets, emphasizing the impact of model scale, domain-specific fine-tuning, and dataset-specific challenges. Larger models such as OpenBioLLM-70B and Med-PaLM-2, consistently outperformed smaller models, showcasing the benefits of extensive training data and computational resources. However, smaller models, like BioMistral-7B, demonstrated competitive performance on specific datasets, suggesting their potential for resource-constrained environments. Beyond accuracy, we explore the broader implications of LLMs in healthcare, including their applications in medical diagnosis, patient care, clinical decision support, and drug discovery. Despite their promise, LLMs face critical challenges, such as the need for explainability, robust data security, bias mitigation, and hallucination reduction. We conclude that while challenges remain, LLMs hold significant potential to transform healthcare by enhancing efficiency, improving patient outcomes, and facilitating advancements in medical research. Addressing the limitations and promoting responsible innovation will be essential to unlocking their full potential for a patient-centered and equitable healthcare future.", "journal": "Health information science and systems", "year": "2026"}
{"pmid": "41270457", "title": "Large language models (LLMs) in radiography research: A narrative review.", "abstract": "Artificial intelligence (AI) has become increasingly embedded in Radiography research and practice, extending beyond diagnostic support and workflow optimisation to non-patient-facing applications. Generative AI (GenAI), particularly Large Language Models (LLMs), have been used in radiography research, generating synthetic data, assisting in literature reviews, and facilitating multilingual communication. This narrative review aims to explore the opportunities, challenges, and implications of LLM integration in radiography research. A narrative review approach was employed, identifying relevant publications and integrating author-led examples, including survey design, translation support, and workflow integration. The literature shows that LLMs accelerate research processes by supporting systematic literature retrieval, enhancing survey development, generating synthetic imaging data, streamlining data analysis and enhancing internationalisation. Case studies highlighted measurable benefits, such as improved CT image quality and reduced examination times through AI-assisted communication. However, challenges included hallucinated outputs, embedded biases, risks to privacy, regulatory challenges and environmental costs of model training. GenAI and LLMs offer transformative opportunities for radiography research across multiple stages, from study design to dissemination. Nonetheless, integration must be accompanied by validation against expert-reviewed datasets, transparent reporting, and ethical safeguards to ensure reliability. Radiography researchers should adopt GenAI tools with a \"trust but verify\" approach: leveraging their efficiency while verifying outputs through expert oversight. Training, governance, and validation frameworks are essential for safe implementation, ensuring these technologies augment rather than replace human expertise in radiography research.", "journal": "Radiography (London, England : 1995)", "year": "2025"}
{"pmid": "41270264", "title": "Evaluating the Effectiveness of Generative AI for the Creation of Patient Education Materials on Coronary Heart Disease: A Comparative Study.", "abstract": "Generative artificial intelligence (AI) has shown great potential in various fields, including health care. However, its application for developing patient education materials (PEMs), particularly for those with coronary heart disease (CHD), remains underexplored. Traditional methods for creating these materials are time-consuming and lack personalization, which limit their effectiveness. This study aims to explore the effectiveness of generative AI tools (ChatGPT and DeepSeek) at generating PEMs for patients with CHD and to compare them with materials developed by a professional medical team. In February 2025, PEMs for patients with CHD were developed using a framework designed by a professional medical team. Structured prompts were used to generate materials through 2 generative AI models-ChatGPT-4o and DeepSeek R1. These AI-generated materials were compared with those created by the medical team in terms of development time, readability, understandability, actionability, and accuracy. The total time for manual preparation was 14 hours, while ChatGPT and DeepSeek consumed 0.62 hours and 0.78 hours, respectively. Regarding readability, the frequency of difficult words was more variable in manually written and ChatGPT materials, while DeepSeek showed more consistency. The proportion of simple sentences was highest with DeepSeek, followed by ChatGPT, with complete separation between manually written and ChatGPT (\u03b4=1). Content word frequency was highest in manually written PEMs, while ChatGPT had the lowest but most stable values. Personal pronouns were most frequently used in manually written PEMs, with high variability, and least used in DeepSeek, which was stable. All 3 methods had similar readability levels and reached Chinese elementary school-level readability for the proportions of simple sentences and personal pronouns, with high school-level difficulty of words and content word frequency. The understandability and actionability scores were above 70, with ChatGPT being more stable for understandability and DeepSeek being more stable for actionability. No significant differences were found between groups. In terms of accuracy, intergroup comparisons showed significant differences (H=7.27, P=.03) but no significant differences in multiple comparisons. The direct comparison between ChatGPT and DeepSeek showed a negligible effect size (\u03b4=0.02), with no significant difference (z-score=-0.06, P=.96). Accuracy issues in the AI-generated materials were noted by 4 of 8 experts. Generative AI significantly improved the efficiency of developing PEMs for patients with CHD. The materials generated by ChatGPT-4o and DeepSeek R1 were comparable to the professionally written ones in terms of readability, understandability, and actionability. However, improvements related to reducing the number of difficult words and increasing content word frequency are needed to enhance readability. The accuracy of AI-generated materials still poses concerns, including potential AI \"hallucinations,\" and requires review by health care professionals. Generative AI holds considerable potential for generating PEMs, and future research should assess its applicability and effectiveness in real-world patient and family contexts.", "journal": "JMIR formative research", "year": "2025"}
{"pmid": "41266194", "title": "Artificial intelligence reliability in implant dentistry: A comparative analysis of clinical accuracy and hallucination patterns across multiple language models.", "abstract": "Artificial intelligence (AI) language models have been increasingly used for clinical decision-making in implant dentistry and for scholarly writing. Yet, their reliability, fabrication rates, and comparative performance across different architectures and generations remain unestablished, potentially compromising evidence-based practice. The purpose of this cross-sectional comparative study was to evaluate the accuracy of different AI models in implant dentistry. How often they produced false information under evidence-based prompting was also examined. The clinical accuracy and hallucination patterns of multiple AI language models, including both conventional large language models (represented by ChatGPT) and retrieval-augmented generation (RAG) systems (represented by ScholarQA and OpenEvidence), were examined with regard to their response to evidence-based clinical questions in implant dentistry and to whether evidence-based prompting strategies reduced fabrication rates across these different AI architectures. Five AI models (GPT-4o, Model 4.1, Model 4.5, ScholarQA, and OpenEvidence) were tested with 15 clinical questions in implant dentistry under both unprompted and evidence-based prompting conditions. ScholarQA and OpenEvidence both achieved high accuracy (80.0%) with 0% fabrication, while GPT-4o showed high rates of reference (82%) and statistical (85.7%) fabrication. More recently released models demonstrated increased fabrication rates compared with earlier versions. Notably, 89.1% of fabricated citations were dated 2023 to 2025. Evidence-based prompting increased GPT-4o's accuracy from 33.3% to 66.7% but did not reduce fabrication rates. Both RAG systems showed minimal response to prompting, with OpenEvidence improving marginally to 86.6% and ScholarQA decreasing to 73.3%. Conventional generative models frequently produced hallucinations, including fabricated citations and data, whereas retrieval-augmented systems such as ScholarQA and OpenEvidence avoided fabricated references but still generated incorrect interpretations. ChatGPT-4.5, ScholarQA, and OpenEvidence demonstrated comparable accuracy, yet hallucination in these forms remains the principal barrier to reliable clinical and scholarly use. Prompting improved performance mainly in earlier generations of AI models, while more recently released versions and RAG systems showed only limited benefit or were unaffected.", "journal": "The Journal of prosthetic dentistry", "year": "2025"}
{"pmid": "41260630", "title": "Learning from the past, structuring the future: using large language models to unlock a century of paediatric research in <i>Archives of Disease in Childhood</i>.", "abstract": "The centenary of <i>Archives of Disease in Childhood</i> (<i>ADC</i>) presents an opportunity to reflect on a century of paediatric research and consider how best to leverage this ever-growing repository for future use. While content is indexed via PubMed and medical subject headings terms, this provides a superficial representation of complex journal content, leading to limited accessibility. We discuss the potential utility of large language models (LLMs)-advanced artificial intelligence systems that can understand, summarise and generate human-like language-and demonstrate their feasibility for structuring historical <i>ADC</i> articles, proposing a future pipeline to enhance indexing, retrieval and discoverability. For demonstrative purposes, five articles from <i>ADC</i> December 1999 issue were locally downloaded and processed using a closed deployment of an LLM, Mistral (V.0.3, 7B). A structured prompt was used to extract key metadata. Outputs were manually compared with source texts and scored for accuracy. Hallucinations, fabricated or incorrect outputs, were recorded. The LLM achieved a mean accuracy of 86.9%, aligning with previous benchmarks for medical research assistance. No hallucinations were identified. Some repetition and verbosity were noted, likely due to chunk-based processing, but key fields were accurately extracted when explicitly present. <i>ADC</i> holds a vast but underutilised body of research. This article shows that lightweight, locally hosted LLMs could structure <i>ADC</i> content without compromising intellectual property. Such methods could enable improved access, support automation of systematic reviews and enhance discoverability through biomedical ontologies, laying the foundation for a searchable, semantically enriched <i>Archive</i>s that bridges historical insight with modern research needs.", "journal": "Archives of disease in childhood", "year": "2025"}
{"pmid": "41244595", "title": "Retrieval-augmented generation for interpreting clinical laboratory regulations using large language models.", "abstract": "Large language models (LLMs) have demonstrated strong performance on general knowledge tasks, but they have important limitations as standalone tools for question answering in specialized domains where accuracy and consistency are critical. Retrieval-augmented generation (RAG) is a strategy in which LLM outputs are grounded in dynamically retrieved source documents, offering advantages in accuracy, explainability, and maintainability. We developed and evaluated a custom RAG system called Raven, designed to answer laboratory regulatory questions using the part of the Code of Federal Regulations (CFR) pertaining to laboratory (42 CFR Part 493) as an authoritative source. Raven employed a vector search pipeline and a LLM to generate grounded responses via a chatbot-style interface. The system was tested using 103 synthetic laboratory regulatory questions, 88 of which were explicitly addressed in the CFR. Compared to answers generated manually by a board-certified pathologist, Raven's responses were judged to be totally complete and correct in 92.0% of those 88 cases, with little irrelevant content and a low potential for regulatory or medical error. Performance declined significantly on questions not addressed in the CFR, confirming the system's grounding in the source documents. Most suboptimal responses were attributable to faulty source document retrieval rather than model hallucination or misinterpretation. These findings demonstrate that a basic RAG system can produce useful, accurate, and verifiable answers to complex regulatory questions. With appropriate safeguards and with thoughtful integration into user workflows, tools like Raven may serve as valuable decision-support systems in laboratory medicine and other knowledge-intensive healthcare domains.", "journal": "Journal of pathology informatics", "year": "2025"}
{"pmid": "41236204", "title": "Enhancing Large Language Models With AI Agents for Chronic Gastritis Management: Comprehensive Comparative Study.", "abstract": "The prevalence of chronic gastritis is high, and if not intervened in a timely manner, it may eventually lead to gastric cancer. Managing chronic gastritis essentially requires comprehensive lifestyle changes. However, the current health care environment does not support continuous follow-up by professional health care providers, making self-management a key component of postdiagnosis care. Increasingly, researchers are exploring the use of large language models (LLMs) for patient management. However, LLMs have limitations, including hallucinations, limited knowledge scope, and lack of timeliness. Artificial intelligence (AI) agents may provide a more effective solution. Nevertheless, it remains uncertain whether AI agents can effectively support postdiagnosis self-management for patients with chronic gastritis. The purpose of this study was to explore the effectiveness of AI agents in the postdiagnosis management of patients with chronic gastritis from different perspectives. In this study, we developed an agent framework for the health management of patients with chronic gastritis based on LLMs in conjunction with retrieval-augmented generation and a search engine tool. We collected real questions from patients with chronic gastritis in clinical settings and tested the framework's performance across different difficulty levels and scenarios. We analyzed its safety and robustness and compared it with state-of-the-art models to comprehensively evaluate its effectiveness. Using a dual-evaluation framework comprising automated metrics and expert manual assessments, our results demonstrated that AI agents substantially outperformed LLMs in addressing high-complexity questions (embedding average score: 82.849 for AI agents vs 77.825 for LLMs) and were particularly effective in clinical consultation tasks. Clinical evaluation of safety based on a 5-point Likert scale by physicians indicated that the safety of the agents was 4.98 (SD 0.15; 95% CI 4.96-4.99). After 30 repeated experiments, the mean absolute deviation of the AI agents in the embedding average score and BERTScore metrics were 0.0167 and 0.0387, respectively. Therefore, the safety and robustness analysis confirmed that the AI agents can produce safe, stable, and minimally variable responses. In addition, comparative results with those of advanced medical-domain LLMs (Baichuan-14B-M1 and MedGemma-27B) and general-domain LLMs (Qwen3-32B) also demonstrated that the AI agents in this study performed outstandingly in the field of chronic gastritis. Our findings underscore the superior reliability, interpretability, and practical applicability of AI agents over conventional LLMs in chronic gastritis management, offering a robust foundation for their broader adoption in health care settings. AI agents based on LLMs have high application value in the management of chronic gastritis. They can effectively guide patients with chronic diseases in addressing common issues, which may potentially reduce the workload of physicians and improve the quality of patient home care.", "journal": "JMIR medical informatics", "year": "2025"}
{"pmid": "41234459", "title": "AI performance in emergency medicine fellowship examination: comparative analysis of ChatGPT-4o, Gemini 2.0, Claude 3.5, and DeepSeek R1 models.", "abstract": "This study evaluated the accuracy rates and response consistency of four different large language models (ChatGPT-4o, Gemini 2.0, Claude 3.5, and DeepSeek R1) in answering questions from the Emergency Medicine Fellowship Examination (YDUS), which was administered for the first time in T\u00fcrkiye. In this observational study, 60 multiple-choice questions from the Emergency Medicine YDUS administered on 15 December 2024, were classified as knowledge-based (n = 26), visual content (n = 2), and case-based (n = 32). Each question was presented three times to the four large language models. The models' accuracy rates were evaluated according to overall accuracy, strict accuracy, and ideal accuracy criteria. Response consistency was measured using Fleiss' Kappa test. The ChatGPT-4o model was the most successful in terms of overall accuracy (90.0%), while DeepSeek R1 showed the lowest performance (76.7%). Claude 3.5 (83.3%) and Gemini 2.0 (80.0%) demonstrated moderate success. When analyzed by category, ChatGPT-4o achieved the highest success with 92.3% accuracy in knowledge-based questions and 90.6% in case-based questions. In terms of response consistency, the Claude 3.5 model (Fleiss' Kappa = 0.68) showed the highest consistency, while Gemini 2.0 (Fleiss' Kappa = 0.49) showed the lowest. Inconsistent hallucinations were more frequent in the Gemini 2.0 and DeepSeek R1 models, whereas persistent hallucinations were less common in the ChatGPT-4o and Claude 3.5 models. Large language models can achieve high accuracy rates for knowledge and clinical reasoning questions in emergency medicine but show differences in terms of response consistency and hallucination tendency. While these models have significant potential for use in medical education and as clinical decision support systems (CDSS), they need further development to provide reliable, up-to-date, and accurate information.", "journal": "Turkish journal of medical sciences", "year": "2025"}
{"pmid": "41224539", "title": "Performance of Large Language Models on Radiology Residency In-Training Examination Questions.", "abstract": "Large language models (LLMs) are increasingly investigated in radiology education. This study evaluated the performance of several advanced LLMs on radiology residency in-training examination questions, with a focus on whether recently released versions show improved accuracy compared with earlier models. We analyzed 282 multiple-choice questions (191 text-only, 91 image-based) from institutional radiology residency examinations conducted between 2023 and 2025. Five LLMs were tested: ChatGPT-4o, ChatGPT-5, Claude 4 Opus, Claude 4.1 Opus, and Gemini 2.5 Pro. Radiology resident performance on the same set of questions was also analyzed for comparison. Accuracy rates were calculated for overall, text-only, and image-based questions, and results were compared using Cochran's Q and Bonferroni-adjusted McNemar tests. Outputs were also assessed for hallucinations. Gemini 2.5 Pro achieved the highest overall accuracy (83.0%), followed by ChatGPT-5 (82.3%). By comparison, radiology residents achieved an overall accuracy of 78.2%. ChatGPT-5 showed significantly higher accuracy compared with ChatGPT-4o (p = 0.021), and Gemini 2.5 Pro showed significantly higher accuracy compared with Claude 4 Opus (p = 0.026). For text-only questions, the highest accuracy was obtained with Gemini 2.5 Pro (88.0%). For image-based questions, radiology residents achieved the highest accuracy (80.4%), followed by ChatGPT-5 (73.6%). The highest accuracies by subspecialty were observed in interventional radiology and physics, whereas breast imaging yielded the lowest accuracy across the models. No instances of hallucination were observed. LLMs demonstrated generally good performance on radiology residency assessments, with newer versions showing measurable improvements. However, limitations persist in image-based interpretation and certain subspecialties. LLMs should therefore be regarded as supportive resources in radiology education, with careful validation and continued refinement of medical training data.", "journal": "Academic radiology", "year": "2025"}
{"pmid": "41223783", "title": "Evaluation of large language models in the clinical management of multiple chronic conditions: A comparative study of DeepSeek-R1 and ChatGPT-o1.", "abstract": "To systematically compare the performance of two mainstream large language models (LLMs), DeepSeek-R1 and ChatGPT-o1, in addressing clinical questions related to multiple chronic conditions (MCCs). Comparative evaluation study. Forty-eight clinical questions related to MCCs, covering varying levels of difficulty, were developed based on authoritative clinical guidelines and independently submitted to DeepSeek-R1 and ChatGPT-o1 in three separate rounds. A panel of five experts in clinical medicine and medical informatics conducted blinded evaluations of 288 de-identified responses across four dimensions: accuracy, comprehensiveness, clarity, and relevance. Statistical differences between models were analyzed using the Mann-Whitney U test and the Kruskal-Wallis H test. Inter-rater reliability was assessed with Fleiss' kappa. Inter-rater reliability was high (Fleiss'\u00a0kappa\u00a0=\u00a00.878, P\u00a0<\u00a00.05). While both models avoided major factual errors, ChatGPT-o1 significantly outperformed DeepSeek-R1 in all four evaluation dimensions. DeepSeek-R1, however, showed greater internal consistency across the three rounds of responses (Fleiss' kappa\u00a0=\u00a00.635, P\u00a0<\u00a00.05). ChatGPT-o1 demonstrated superior performance in MCC-related clinical tasks, particularly in complex cases. Nevertheless, both models remain susceptible to hallucinations, underscoring the need to enhance clinical accuracy and interpretability. This study provides empirical evidence for the application of LLMs in MCC management and highlights the importance of improving model reliability, standardizing evaluation frameworks, and exploring multi-model integration to support their safe and effective use in clinical settings.", "journal": "Public health", "year": "2025"}
{"pmid": "41219270", "title": "Limitations of large language models in clinical problem-solving arising from inflexible reasoning.", "abstract": "Large Language Models (LLMs) have attained human-level accuracy on medical question-answer (QA) benchmarks. However, their limitations in navigating clinical scenarios requiring flexible reasoning have recently been shown, raising concerns about the robustness and generalizability of LLM reasoning across diverse, real-world medical tasks. To probe potential LLM failure modes in clinical problem-solving, we present the medical abstraction and reasoning corpus (mARC-QA). mARC-QA assesses clinical reasoning through scenarios designed to exploit the Einstellung effect-the fixation of thought arising from prior experience, targeting LLM inductive biases toward inflexible pattern matching from their training data rather than engaging in flexible reasoning. We find that LLMs, including current state-of-the-art o1, Gemini, Claude, and DeepSeek models, perform poorly compared to physicians on mARC-QA, often demonstrating lack of commonsense medical reasoning and a propensity to hallucinate. In addition, uncertainty estimation analyses indicate that LLMs exhibit overconfidence in their answers, despite their limited accuracy. The failure modes revealed by mARC-QA in LLM medical reasoning underscore the need to exercise caution when deploying these models in clinical settings.", "journal": "Scientific reports", "year": "2025"}
{"pmid": "41218640", "title": "AI Prompt Engineering for Neurologists and Trainees.", "abstract": "Large language models (LLMs) have transformative potential in neurology, impacting clinical decision-making, medical training, and research. Prompt engineering, the strategic design of inputs to optimize LLM performance, is essential for neurologists and trainees seeking to effectively integrate these powerful tools into practice. Carefully crafted prompts enable LLMs to summarize complex patient narratives, generate differential diagnoses, and support patient education. In training, structured prompts enhance diagnostic reasoning, board preparation, and interactive case-based learning. Neurological research also benefits, with LLMs aiding in data extraction, computed phenotype generation, and literature synthesis. Despite their promise, challenges remain, including hallucinations, data bias, privacy concerns, and regulatory complexities. This review synthesizes current advances and highlights best practices, including two structured prompt engineering frameworks tailored to neurology: Role-Task-Format (RTF) for routine use and our newly developed BRAIN (Background, Role, Aim, Instructions, Next steps) for complex tasks. We offer practical guidance to maximize accuracy, safety, and equity in LLM outputs, ensuring reliable support for neurologists and trainees.", "journal": "Seminars in neurology", "year": "2025"}
{"pmid": "41213532", "title": "[AI Application in Nephrological Diagnostics].", "abstract": "Artificial intelligence (AI) is rapidly reshaping medical diagnostics, and nephrology - characterized by multifactorial disease patterns - stands to benefit markedly. Machine\u2011learning and deep\u2011learning algorithms, especially convolutional neural networks (CNNs) and large language models (LLMs), can analyze heterogeneous data streams from electronic health records, imaging, histopathology and genomics to support diagnosis, prognosis and therapeutic planning. AI\u2011driven automation of routine workflows (e.g., appointment scheduling, NLP\u2011based documentation, chatbot\u2011guided anamneses) enables clinicians to focus on complex decision\u2011making, while real\u2011time decision\u2011support tools can integrate laboratory, imaging and guideline data. Recent advances include CNN\u2011based detection of renal lesions, deep\u2011learning prognostic scores for IgA nephropathy, and AI\u2011enhanced variant calling (e.g., DeepVariant). Nevertheless, challenges persist: data bias, limited external validation, \"hallucinations\" of LLMs, regulatory compliance (MDR, GDPR), and the need for transparent, locally hosted models. Successful implementation requires interoperable, FHIR\u2011compatible data, robust training of staff, and integration of AI education into medical education. With this, AI promises substantial efficiency gains, improved diagnostic precision, and sustained care quality in nephrology.", "journal": "Deutsche medizinische Wochenschrift (1946)", "year": "2025"}
{"pmid": "41212429", "title": "Evaluating the Performance of DeepSeek-R1 as a Patient Education Tool.", "abstract": "The cost-effective open-source artificial intelligence (AI) model DeepSeek-R1 in China holds significant potential for healthcare applications. As a health education tool, it could help patients acquire health science knowledge and improve health literacy. Low back pain (LBP), the most common musculoskeletal problem globally, has seen increasing use of large language model (LLM)-based AI chatbots by patients to access health information, making it critical to further examine the quality of such information.\u00a0This study aimed to evaluate the response quality and readability of answers generated by DeepSeek-R1 to common patient questions about LBP.\u00a0Ten questions were formulated using inductive methods based on literature analysis and Baidu Index data, which were presented to DeepSeek-R1 on March 10, 2025. The evaluation spanned readability, understandability, actionability, clinician assessment, and reference assessment. Readability was measured using the Flesch-Kincaid Grade Level, Flesch Reading Ease Scale, Gunning Fog Index, Coleman-Liau Index, and Simple Measure of Gobbledygook (SMOG Index). Understandability and actionability were assessed via the Patient Education Materials and Assessment Tool for Printable Materials (PEMAT-P). Clinicians evaluated accuracy, completeness, and correlation. A reference evaluation tool was used to assess reference quality and the presence of hallucinations.\u00a0Readability analysis indicated that DeepSeek's responses were overall \"difficult to read\", with Flesch-Kincaid Grade Level (mean 12.39, SD 1.91), Flesch Reading Ease Scale (mean 19.55, Q1 12.94, Q3 29.78), Gunning Fog Index (mean 13.95, SD 2.61), Coleman-Liau Index (mean 17.46, SD 2.30), and SMOG Index (mean 11.04, SD 1.37). PEMAT-P revealed good understandability but weak actionability. Consensus among five clinicians confirmed satisfactory accuracy, completeness, and relevance. References Assessment identified 9 instances (14.8%) of hallucinated references, while Supporting was rated as moderate, with most references sourced from authoritative platforms.\u00a0Our study demonstrates the potential of DeepSeek-R1 in the educational content for patients with LBP. It can be employed as a supplement to patient education tools rather than substituting for clinical judgment.", "journal": "Journal of medical systems", "year": "2025"}
{"pmid": "41211069", "title": "Enabling Drug-Induced Liver Injury Surveillance Through Automated Medication Extraction From Clinical Notes: A Medical Information Mart for Intensive Care IV Real-World Large Language Models Validation Study.", "abstract": "Drug-induced liver injury (DILI) presents a significant diagnostic challenge, often leading to delayed detection. Unstructured clinical notes contain comprehensive medication data vital for DILI surveillance but are difficult to analyze systematically. Large language models (LLMs) show promise for automated extraction but require real-world clinical data validation to assess feasibility for clinical applications like DILI surveillance. We retrospectively validated an LLM system on 100 randomly sampled Medical Information Mart for Intensive Care IV (MIMIC-IV) discharge summaries. Gold standard unique medication lists were derived via manual annotation and manual deduplication based on normalized drug names. LLM outputs underwent identical deduplication. Performance was assessed using precision, recall, and F1-score comparing deduplicated lists. MIMIC-IV data use agreement (DUA) compliance was ensured. Comparison yielded a precision of 0.85, recall of 1.00, and an F1-score of 0.92 for unique medication identification. The 174 false positives resulted from parsing or normalization errors; no medication hallucinations occurred. A subsequent DILI database lookup failed for approximately 6.2% of correctly identified unique medications, evaluated as a separate feasibility measure. The LLM demonstrates high accuracy and perfect recall for unique medication extraction and identification from complex clinical notes, establishing technical feasibility. This represents a feasible and possible integration of LLM towards developing automated tools for enhanced DILI surveillance and improved patient safety.", "journal": "Gastroenterology research", "year": "2025"}
{"pmid": "41207799", "title": "ChatGPT and other large language models for childhood asthma.", "abstract": "Large language models (LLMs) such as ChatGPT, Claude, and Gemini have become widely accessible since 2022. As childhood asthma remains the most common chronic paediatric condition with persistent gaps in optimal management, these tools present both opportunities and challenges for families and healthcare professionals. This narrative review examines the role of commercially available LLMs in childhood asthma care, exploring their fundamental principles, current evidence, and potential applications. Studies show that LLMs can generate medically accurate and comprehensible responses to asthma-related queries. Healthcare professionals may also benefit from rapid summarisation and tailored educational content. However, risks include hallucinations, bias, and data privacy concerns. Further research is required to evaluate the safety, clinical utility, and real-world acceptability of LLMs - particularly in acute asthma management by families and in supporting clinical decisions by healthcare professionals - and to guide the development of reliable, inclusive tools tailored to paediatric respiratory care.", "journal": "Paediatric respiratory reviews", "year": "2025"}
{"pmid": "41207288", "title": "ChatGPT: how to use it and the pitfalls/cautions in academia.", "abstract": "The integration of large language models (LLMs) in academic research has transformed traditional research methodologies. This review investigates the current state, applications, and limitations of LLMs, particularly ChatGPT, in medical and scientific research. I performed a systematic review of recent literature and LLM development reports in artificial intelligence-assisted research tools, including commercial LLM services (GPT-4o, Claude 3, Gemini Pro) and specialized research platforms (Genspark, Scispace). I evaluated their performance, applications, and limitations across stages of the research process. Recent advancements in LLMs shows potential for improving research efficiency, particularly in literature review, data analysis, and manuscript preparation. Performance comparison revealed varying strengths: GPT-4o and o1 outperformed in the overall area, Claude 3 in writing and coding, and Gemini Pro in multimodal processing. Therefore, it is important to choose and use each model wisely according to its advantages. However, hallucination risks, inherent biases, plagiarism concerns, and privacy issues are concerns in LLMs. The emergence of Retrieval-Augmented Generation models and specialized research tools has improved accuracy and current information access. LLMs offer effective support for research productivity, but they should serve as complementary tools rather than primary research drivers. The successful application of these tools depends on a thorough understanding of their limitations, strict adherence to ethical guidelines, and preservation of researcher autonomy.", "journal": "Annals of pediatric endocrinology & metabolism", "year": "2025"}
{"pmid": "41206112", "title": "BioWorkflow: Retrieving comprehensive bioinformatics workflows from publications.", "abstract": "Reconstructing bioinformatics workflows from the literature is the foundation of scientific analysis. However, the required details-processing steps, software tools, versions, and parameter settings-are dispersed across narrative text, tables, figure captions, and supplemental files. Manual reconstruction typically takes hours per paper and is error-prone, while existing question-answering (QA) and retrieval systems focus on local passages and lack the full-text, multimodal capabilities needed to automatically rebuild complete workflows. We introduce BioWorkflow, a large language model (LLM)-based, retrieval-augmented framework that automates end-to-end workflow extraction from publications by (i) parsing PDFs and building a unified index over text, tables, and figures with chunk-level summaries and embeddings; (ii) hierarchically decomposing queries with dynamic reformulation when new entities or ambiguities emerge; (iii) performing iterative, context-aware retrieval and assembling a directed workflow that captures steps, tools, versions, and parameters; and (iv) linking each predicted element to its cited evidence and running automated consistency checks to suppress hallucinations and ensure traceability. Evaluated on 100 expert-annotated papers, BioWorkflow recovers ~80% of workflow steps (versus ~20% for existing tools), improves reproducibility, completeness, and accuracy by >20% over strong LLM baselines, and reduces curation time to 3-5\u00a0minutes per paper, enabling rapid and reliable reuse of published pipelines.", "journal": "Briefings in bioinformatics", "year": "2025"}
{"pmid": "41199808", "title": "Assessing the quality of AI-generated clinical notes: validated evaluation of a large language model ambient scribe.", "abstract": "Generative artificial intelligence (AI) tools are increasingly being used as \"ambient scribes\" to generate drafts for clinical notes from patient encounters. Despite rapid adoption, few studies have systematically evaluated the quality of AI-generated documentation against physician standards using validated frameworks. This study aimed to compare the quality of large language model (LLM)-generated clinical notes (\"Ambient\") with physician-authored reference (\"Gold\") notes across five clinical specialties using the Physician Documentation Quality Instrument (PDQI-9) as a validated framework to assess document quality. We pooled 97 de-identified audio recordings of outpatient clinical encounters across general medicine, pediatrics, obstetrics/gynecology, orthopedics, and adult cardiology. For each encounter, clinical notes were generated using both LLM-optimized \"Ambient\" and blinded physician-drafted \"Gold\" notes, based solely on audio recording and corresponding transcripts. Two blinded specialty reviewers independently evaluated each note using the modified PDQI-9, which includes 11 criteria rated on a Likert-scale, along with binary hallucination detection. Interrater reliability was assessed using within-group interrater agreement coefficient (RWG) statistics. Paired comparisons were performed using <i>t</i>-tests or Mann-Whitney tests. Paired analysis of 97 clinical encounters yielded 194 notes (2 per encounter) and 388 paired reviews. Overall, high interrater agreement was observed (RWG\u202f>\u202f0.7), with moderate concordance noted in pediatrics and cardiology. Gold notes achieved higher overall quality scores (4.25/5 vs. 4.20/5, <i>p</i>\u202f=\u202f0.04), as well as superior accuracy (<i>p</i>\u202f=\u202f0.05), succinctness (<i>p</i>\u202f<\u202f0.001), and internal consistency (<i>p</i>\u202f=\u202f0.004) compared to ambient notes. In contrast, ambient notes scored higher in thoroughness (<i>p</i>\u202f<\u202f0.001) and organization (<i>p</i>\u202f=\u202f0.03). Hallucinations were detected in 20% of gold notes and 31% of ambient notes (<i>p</i>\u202f=\u202f0.01). Despite these limitations, reviewers overall preferred ambient notes (47% vs. 39% for gold). LLM-generated Ambient notes demonstrated quality comparable to physician-authored notes across multiple specialties. While Ambient notes were more thorough and better organized, they were also less succinct and more prone to hallucination. The PDQI-9 provides a validated, practical framework for evaluating AI-generated clinical documentation. This quality assessment methodology can inform iterative quality optimization and support the standardization of ambient AI scribes in clinical practice.", "journal": "Frontiers in artificial intelligence", "year": "2025"}
{"pmid": "41190890", "title": "Assessing the Limitations of Large Language Models in Clinical Practice Guideline-Concordant Treatment Decision-Making on Real-World Data: Retrospective Study.", "abstract": "Studies have shown that large language models (LLMs) are promising in therapeutic decision-making, with findings comparable to those of medical experts, but these studies used highly curated patient data. This study aimed to determine if LLMs can make guideline-concordant treatment decisions based on patient data as typically present in clinical practice (lengthy, unstructured medical text). We conducted a retrospective study of 80 patients with severe aortic stenosis who were scheduled for either surgical (SAVR; n=24) or transcatheter aortic valve replacement (TAVR; n=56) by our institutional heart team in 2022. Various LLMs (BioGPT, GPT-3.5, GPT-4, GPT-4 Turbo, GPT-4o, LLaMA-2, Mistral, PaLM 2, and DeepSeek-R1) were queried using either anonymized original medical reports or manually generated case summaries to determine the most guideline-concordant treatment. We measured agreement with the heart team using Cohen \u03ba coefficients, reliability using intraclass correlation coefficients (ICCs), and fairness using the frequency bias index (FBI; FBI >1 indicated bias toward TAVR). When presented with original medical reports, LLMs showed poor performance (Cohen \u03ba coefficient: -0.47 to 0.22; ICC: 0.0-1.0; FBI: 0.95-1.51). The LLMs' performance improved substantially when case summaries were used as input and additional guideline knowledge was added to the prompt (Cohen \u03ba coefficient: -0.02 to 0.63; ICC: 0.01-1.0; FBI: 0.46-1.23). Qualitative analysis revealed instances of hallucinations in all LLMs tested. Even advanced LLMs require extensively curated input for informed treatment decisions. Unreliable responses, bias, and hallucinations pose significant health risks and highlight the need for caution in applying LLMs to real-world clinical decision-making.", "journal": "JMIRx med", "year": "2025"}
{"pmid": "41177245", "title": "Clinical pathway-aware large language models for reliable and transparent medical dialogue.", "abstract": "Large language models (LLMs) offer promising potential in answering real-time medical queries, but they often produce lengthy, generic, and even hallucinatory responses. We aim to develop a reliable and interpretable medical dialogue system that incorporates clinical reasoning and then mitigates the risk of hallucination. Two large datasets of real-world online consultation, MedDG and KaMed, were used for evaluation. We proposed a Medical Dialogue System with Knowledge Enhancement and Clinical Pathway Encoding (MedKP), which integrates an external medical knowledge graph and encodes internal clinical pathways to model physician reasoning. Performance was compared with state-of-the-art baselines, including GPT-4o and LLaMA3.1-70B. A multi-dimensional evaluation framework assessed (1) clinical relevance (medical entity-based), (2) textual similarity (ROUGE, BLEU), (3) semantic alignment (BERTScore), and (4) hallucination and consistency via an external LLM-based judge, as well as parallel human evaluation. Across both datasets, MedKP (6B) achieved the best overall performance, outperforming other advanced baselines and producing responses that align more closely with those of human physicians. For clinical relevance, MedKP reached a macro F1-score of medical entity at 31.41 on MedDG (previous best DFMed: 24.76, improved 30.41%) and 26.62 on KaMed (previous best LLaM-A3.1-70B: 20.67, improved 25.62%). Consistent improvements were observed across other metrics. Ablation studies further validated the effectiveness of each model component. Our results highlight the critical role of clinical reasoning in advancing trustworthy AI for digital healthcare. By enhancing the reliability, coherence, and transparency of AI-generated responses, this pathway-aware approach bridges the gap between LLMs and real-world clinical workflows, improving the accessibility of high-quality telemedicine services, particularly benefiting underserved populations.", "journal": "Journal of biomedical informatics", "year": "2025"}
{"pmid": "41171081", "title": "Medical Feature Extraction From Clinical Examination Notes: Development and Evaluation of a Two-Phase Large Language Model Framework.", "abstract": "Medical feature extraction from clinical text is challenging because of limited data availability, variability in medical terminology, and the critical need for trustworthy outputs. Large language models (LLMs) offer promising capabilities but face critical challenges with hallucination. This study aims to develop a robust framework for medical feature extraction that enhances accuracy by minimizing the risk of hallucination, even with limited training data. We developed a two-phase training approach. Phase 1 used instructing fine-tuning to teach feature extraction. Phase 2 introduced confidence-regularization fine-tuning with loss functions penalizing overconfident incorrect predictions, which were captured using bidirectional matching targeting hallucination and missing features. The model was trained using the full data of 700 patient notes and on few-shot 100 patient notes. We evaluated the framework on the United States Medical Licensing Examination Step-2 Clinical Skills dataset, testing on a public split of 200 patient notes and a private split of 1839 patient notes. Performance was assessed using precision, recall, and F<sub>1</sub>-scores, with error analysis conducted on predicted features from the private test set. The framework achieved an F<sub>1</sub>-score of 0.968-0.983 on the full dataset of 700 patient notes and 0.960-0.973 with a few-shot subset of 100 of 700 patient notes (14.2%), outperforming INCITE (intelligent clinical text evaluator; F<sub>1</sub>=0.883) and DeBERTa (decoding-enhanced bidirectional encoder representations from transformers with disentangled attention; F<sub>1</sub>=0.958). It reduced hallucinations by 89.9% (from 3081 to 311 features) and missing features by 88.9% (from 6376 to 708) on the private dataset compared with the baseline LLM with few-shot in-context learning. Calibration evaluation on few-shot training (100 patient notes) showed that the expected calibration error increased from 0.060 to 0.147, whereas the Brier score improved from 0.087 to 0.036. Notably, the average model confidence remained stable at 0.84 (SD 0.003) despite F<sub>1</sub> improvements from 0.819 to 0.986. Our two-phase LLM framework successfully addresses critical challenges in automated medical feature extraction, achieving state-of-the-art performance while reducing hallucination and missing features. The framework's ability to achieve high performance with minimal training data (F<sub>1</sub>=0.960-0.973 with 100 samples) demonstrates strong generalization capabilities essential for resource-constrained settings in medical education. While traditional calibration metrics show misalignment, the practical benefits of confidence injection led to reduced errors, and inference-time filtering provided reliable outputs suitable for automated clinical assessment applications.", "journal": "JMIR medical informatics", "year": "2025"}
{"pmid": "41162483", "title": "Evaluating the performance of large language models versus human researchers on real world complex medical queries.", "abstract": "Whether large language models (LLMs) can resolve real-world dilemmas faced by clinicians remains unclear and physician quality assessment is often used as a measure of LLM output quality. We compared reports - defined as answers to clinical queries generated by LLMs or written by human researchers - generated by GPT-4o, Gemini 2.0, and Claude Sonnet 3.5 in response to such dilemmas (n = 20) to reports written by trained human researchers and studied whether physician satisfaction correlates with objective report quality. Twenty human reports and fifty-six LLM-reports were analyzed. Human reports met physicians' expectations more frequently (p = 0.044), were considered more reliable (p = 0.032), professionally written (p = 0.003), and time-saving (p = 0.003). Human reports cited more sources (p < 0.001) and while these were from lower ranking journals (median IF: 7 [3, 11] vs 14 [10, 27], p = 0.003), they were considered more relevant (p < 0.001). Unlike LLMs, human reports contained no hallucinated (p < 0.001) or unfaithful (p < 0.001) citations. However, no meaningful correlation was identified between physician satisfaction and objective measures of report quality. A meaningful gap remains between LLM and human capacity to respond reliably and in a relevant manner to real-life clinical dilemmas. Of greater concern is that physician satisfaction with generated content is not a good measure of quality.", "journal": "Scientific reports", "year": "2025"}
{"pmid": "41161918", "title": "Performance of Large Language Models on the Acute Coronary Syndrome Guidelines Using Retrieval-Augmented Generation.", "abstract": "Large language models (LLMs) are increasingly applied in interventional cardiology, but hallucinations limit their clinical utility. The aim of this study was to assess whether retrieval-augmented generation (RAG), a technique that allows LLMs to access guideline content during response generation, improves accuracy when answering questions on the basis of the guidelines for acute coronary syndromes. The accuracy of ChatGPT-4o, DeepSeek R1, and Med-PaLM 2 was compared using a set of 38 open-ended cardiology guideline-based questions and answers. ChatGPT-4o and DeepSeek R1 were evaluated with and without RAG, while Med-PaLM 2, a medicine-specific LLM, was tested without RAG. Model outputs were compared against guideline recommendations using an artificial intelligence-powered similarity score tool. DeepSeek R1 with RAG achieved the highest accuracy (94.7%; 95% CI: 82.7%-98.5%), followed by ChatGPT-4o with RAG (92.1%; 95% CI: 79.2%-97.3%) (P = 0.922). ChatGPT-4o without RAG achieved 71.1% accuracy (95% CI: 55.2%-83.0%), which significantly improved with RAG (P = 0.017). Among non-RAG models, DeepSeek R1 demonstrated the highest accuracy (78.9%; 95% CI: 63.7%-88.9%), followed by ChatGPT-4o without RAG (71.1%) (P = 0.083). Med-PaLM 2 had the lowest accuracy (68.4%; 95% CI: 52.5%-80.9%). Spearman correlation analysis revealed a strong correlation between DeepSeek R1 without RAG and Med-PaLM 2 (r = 0.646; 95% CI: 0.411-0.800; P < 0.001), indicating similar response patterns. Scatterplot analysis further revealed that RAG disproportionately improved lower scoring questions in DeepSeek R1 while improving scores more evenly in ChatGPT-4o. Embedding guideline content into LLM workflows via RAG can enhance LLM accuracy for clinical applications, particularly in scenarios common to interventional cardiology. These results support the potential for LLMs, when enhanced with domain-specific knowledge, to optimize clinical decision making and increase alignment with guidelines.", "journal": "JACC. Cardiovascular interventions", "year": "2025"}
